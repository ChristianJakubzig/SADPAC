# pages/2_üí¨_Chat.py
import streamlit as st
import logging
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from app.chroma_client import get_chroma_vectorstore
from app.config import Config
from langchain_ollama import OllamaEmbeddings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(
    page_title="Chat",
    page_icon="üí¨",
    layout="wide"
)

# Session State initialisieren
if "selected_collection" not in st.session_state:
    st.session_state.selected_collection = Config.DOCUMENTS_COLLECTION
if "messages" not in st.session_state:
    st.session_state.messages = {}  # Dictionary: collection_name -> messages

@st.cache_resource
def get_embedding_model():
    """Erstellt Ollama Embedding Model (gecached)"""
    return OllamaEmbeddings(
        base_url=Config.OLLAMA_BASE_URL,
        model=Config.OLLAMA_EMBEDDING_MODEL
    )

def get_vectorstore_for_collection(collection_name: str):
    """Erstellt Vectorstore f√ºr spezifische Collection"""
    embedding_model = get_embedding_model()
    return get_chroma_vectorstore(embedding_model, collection_name=collection_name)

st.title("üí¨ RAG Chat")

# Sidebar: Collection-Auswahl und Einstellungen
with st.sidebar:
    st.header("üìö Collection")
    
    # Collection-Auswahl
    collections = [Config.DOCUMENTS_COLLECTION, Config.METADATA_COLLECTION]
    
    selected_collection = st.selectbox(
        "W√§hle Collection",
        collections,
        index=collections.index(st.session_state.selected_collection),
        help="W√§hle zwischen Dokumenten und Metadaten"
    )
    
    # Wenn Collection ge√§ndert wurde
    if selected_collection != st.session_state.selected_collection:
        st.session_state.selected_collection = selected_collection
        st.rerun()
    
    # Hole Vectorstore f√ºr gew√§hlte Collection
    try:
        vectorstore = get_vectorstore_for_collection(selected_collection)
        doc_count = vectorstore._collection.count()
        
        st.metric("üìÑ Dokumente", doc_count)
        st.success("‚úÖ Verbunden")
        
    except Exception as e:
        st.error(f"‚ùå Verbindungsfehler: {e}")
        doc_count = 0
    
    st.divider()
    
    # Such-Einstellungen
    st.header("‚öôÔ∏è Einstellungen")
    
    k_results = st.slider("Anzahl Ergebnisse", 1, 10, 3)
    
    show_sources = st.checkbox("Quellen anzeigen", value=True)
    show_scores = st.checkbox("Relevanz-Scores anzeigen", value=False)
    
    st.divider()
    
    # Collection-Info
    st.subheader("‚ÑπÔ∏è Collection Info")
    st.caption(f"**Name:** {selected_collection}")
    st.caption(f"**Dokumente:** {doc_count}")
    
    st.divider()
    
    if st.button("üóëÔ∏è Chat l√∂schen"):
        if selected_collection in st.session_state.messages:
            st.session_state.messages[selected_collection] = []
        st.rerun()

# Warnung wenn keine Dokumente
if doc_count == 0:
    st.warning(f"‚ö†Ô∏è Keine Dokumente in Collection '{selected_collection}'!")
    st.info("üëâ Gehe zur **Dokumente**-Seite um PDFs hochzuladen oder nutze das Load-Script.")
    
    with st.expander("üí° Dokumente laden"):
        st.code(f"""
# Via Script:
python src/scripts/load_documents.py \\
  --folder data/documents \\
  --collection {selected_collection} \\
  --batch-size 5

# Via Makefile:
make load-docs      # F√ºr Dokumente
make load-metadata  # F√ºr Metadaten
        """, language="bash")
    st.stop()

# Chat-Historie f√ºr aktuelle Collection initialisieren
if selected_collection not in st.session_state.messages:
    st.session_state.messages[selected_collection] = []

# Chat-Historie anzeigen
for message in st.session_state.messages[selected_collection]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Zeige Quellen falls vorhanden
        if "sources" in message and show_sources and message["sources"]:
            with st.expander("üìö Quellen"):
                for i, source in enumerate(message["sources"], 1):
                    st.markdown(f"**{i}.** {source['text'][:200]}...")
                    if show_scores:
                        st.caption(f"‚≠ê Relevanz: {source['score']:.3f} | üìÑ {source['filename']}")
                    else:
                        st.caption(f"üìÑ {source['filename']}")

# Chat-Input
if prompt := st.chat_input(f"Stelle eine Frage zu '{selected_collection}'..."):
    # User-Nachricht anzeigen und speichern
    st.session_state.messages[selected_collection].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Bot-Antwort generieren
    with st.chat_message("assistant"):
        with st.spinner("üîç Durchsuche Dokumente..."):
            try:
                # Similarity Search
                results_with_score = vectorstore.similarity_search_with_score(prompt, k=k_results)
                
                if not results_with_score:
                    response = f"‚ùå Keine relevanten Dokumente in '{selected_collection}' gefunden."
                    sources = []
                else:
                    # Erstelle Antwort
                    response = f"üìä Ich habe **{len(results_with_score)}** relevante Textabschnitte in '{selected_collection}' gefunden:\n\n"
                    
                    sources = []
                    for i, (doc, score) in enumerate(results_with_score, 1):
                        # Sammle Quellen
                        sources.append({
                            "text": doc.page_content,
                            "score": score,
                            "filename": doc.metadata.get('filename', 'Unbekannt')
                        })
                    
                    response += "\n\nüí° **Hinweis:** Die vollst√§ndige LLM-Integration kommt im n√§chsten Schritt. "
                    response += "Aktuell zeige ich dir die relevantesten Textabschnitte."
                
                st.markdown(response)
                
                # Zeige Quellen inline
                if sources and show_sources:
                    with st.expander("üìö Gefundene Quellen", expanded=True):
                        for i, source in enumerate(sources, 1):
                            st.markdown(f"### üìÑ Quelle {i}")
                            
                            # Zeige Text in scrollbarem Container
                            st.text_area(
                                f"Text {i}",
                                source['text'],
                                height=150,
                                key=f"source_{i}",
                                label_visibility="collapsed"
                            )
                            
                            # Metadaten
                            col1, col2 = st.columns(2)
                            with col1:
                                if show_scores:
                                    st.caption(f"‚≠ê Relevanz: {source['score']:.3f}")
                            with col2:
                                st.caption(f"üìÑ {source['filename']}")
                            
                            if i < len(sources):
                                st.divider()
                
                # Speichere Antwort
                st.session_state.messages[selected_collection].append({
                    "role": "assistant",
                    "content": response,
                    "sources": sources
                })
                
            except Exception as e:
                error_msg = f"‚ùå Fehler bei der Suche: {e}"
                st.error(error_msg)
                logger.error(f"Search error: {e}", exc_info=True)
                st.session_state.messages[selected_collection].append({
                    "role": "assistant",
                    "content": error_msg
                })

# Footer
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    st.caption(f"üìö Collection: {selected_collection}")
with col2:
    st.caption(f"üìä {doc_count} Dokumente")
with col3:
    st.caption(f"ü§ñ {Config.OLLAMA_EMBEDDING_MODEL}")